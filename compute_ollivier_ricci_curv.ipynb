{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "compute_ollivier_ricci_curv.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJi7j7BQ3sti"
      },
      "source": [
        "# Project: Node Prediction for OGB-Arxiv using Curvature Graph Neural Networks\n",
        "\n",
        "**CS224W: Machine Learning with Graphs**\n",
        "\n",
        "\n",
        "_Stanford University. Winter, 2021._\n",
        "\n",
        "---\n",
        "\n",
        "**Team Members:** Gongqi Li, Khushal Sethi, Prathyusha Burugupalli\n",
        "\n",
        "---\n",
        "This colab implements generate Ollivier Curvature information for Ogb-Arxiv dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xygEAVNBxaa"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cs4kSkwKDblG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2959afd-7343-422c-ed26-0d3ade2e3891"
      },
      "source": [
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install -q torch-geometric\n",
        "!pip install ogb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.6MB 2.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.5MB 2.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 194kB 5.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 235kB 7.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.2MB 8.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 5.2MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ogb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/47/16573587124ee85c8255cebd30c55981fa78c815eaff966ff111fb11c32c/ogb-1.3.0-py3-none-any.whl (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (0.22.2.post1)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.8.0+cu101)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.1.5)\n",
            "Collecting outdated>=0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/86/70/2f166266438a30e94140f00c99c0eac1c45807981052a1d4c123660e1323/outdated-0.2.0.tar.gz\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (4.41.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.15.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.24.3)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.2.0->ogb) (3.7.4.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2.8.1)\n",
            "Collecting littleutils\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/b1/bb4e06f010947d67349f863b6a2ad71577f85590180a935f60543f622652/littleutils-0.2.2.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2020.12.5)\n",
            "Building wheels for collected packages: outdated, littleutils\n",
            "  Building wheel for outdated (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for outdated: filename=outdated-0.2.0-cp37-none-any.whl size=4962 sha256=48fd63840d38db0bd85267dece22095765be1c65dd0e1db661ae0d7975e8cff2\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/7c/ef/814f514d31197310872b5abf353feb8fef9d67ee658e1e7e39\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-cp37-none-any.whl size=7051 sha256=d6275b9a73e2116f83c220df08aecd99a1648d4fb4afee431be2235ff82f5118\n",
            "  Stored in directory: /root/.cache/pip/wheels/53/16/9f/ac67d15c40243754fd73f620e1b9b6dedc20492ecc19a2bae1\n",
            "Successfully built outdated littleutils\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.0 outdated-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccmR2WOf_RTg",
        "outputId": "d826d2c3-fcce-4a72-dca0-7e75559e4c04"
      },
      "source": [
        "!pip install POT\n",
        "!pip install path.py\n",
        "!pip3 install cmake cython\n",
        "!pip3 install networkit"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting POT\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/47/1ead874bd6ac538f246dcabfe04d3cc67ec94cf8fc8f952ff4e56c07cfe2/POT-0.7.0-cp37-cp37m-manylinux2010_x86_64.whl (430kB)\n",
            "\u001b[K     |████████████████████████████████| 440kB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from POT) (1.19.5)\n",
            "Requirement already satisfied: cython>=0.23 in /usr/local/lib/python3.7/dist-packages (from POT) (0.29.22)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from POT) (1.4.1)\n",
            "Installing collected packages: POT\n",
            "Successfully installed POT-0.7.0\n",
            "Collecting path.py\n",
            "  Downloading https://files.pythonhosted.org/packages/8f/04/130b7a538c25693c85c4dee7e25d126ebf5511b1eb7320e64906687b159e/path.py-12.5.0-py3-none-any.whl\n",
            "Collecting path\n",
            "  Downloading https://files.pythonhosted.org/packages/d3/2a/b0f97e1b736725f6ec48a8bd564ee1d1f3f945bb5d39cb44ef8bbe66bd14/path-15.1.2-py3-none-any.whl\n",
            "Installing collected packages: path, path.py\n",
            "Successfully installed path-15.1.2 path.py-12.5.0\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.7/dist-packages (3.12.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (0.29.22)\n",
            "Collecting networkit\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/58/7a/4ef04f2b34fc81c5f2a5060b5cb989509f847d8aa6e359899922acffacf8/networkit-8.1.tar.gz (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from networkit) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from networkit) (1.19.5)\n",
            "Building wheels for collected packages: networkit\n",
            "  Building wheel for networkit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for networkit: filename=networkit-8.1-cp37-cp37m-linux_x86_64.whl size=8070800 sha256=9b331b62f4a3ce59457230bb91f81e4e19e9a63ff307fcfe1575688ac4a3768e\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/7e/59/6b58f6984b823663f6d01266ecb0306ce3d74d040acd27a818\n",
            "Successfully built networkit\n",
            "Installing collected packages: networkit\n",
            "Successfully installed networkit-8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6C7kXwcQB_Dd"
      },
      "source": [
        "import torch\n",
        "from torch.nn import Sequential as seq, Parameter, LeakyReLU, init, Linear\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "import torch_geometric\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn import GCNConv, SAGEConv, MessagePassing\n",
        "from torch_geometric.utils import add_self_loops, remove_self_loops, degree, softmax\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.utils import to_undirected\n",
        "\n",
        "\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX7ugwpJdRkl"
      },
      "source": [
        "dataset = PygNodePropPredDataset(name='ogbn-arxiv')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXvoV2wYqZay"
      },
      "source": [
        "def to_networkx(data, node_attrs=None, edge_attrs=None, to_undirected=False,\n",
        "                remove_self_loops=False):\n",
        "    r\"\"\"Converts a :class:`torch_geometric.data.Data` instance to a\n",
        "    :obj:`networkx.Graph` if :attr:`to_undirected` is set to :obj:`True`, or\n",
        "    a directed :obj:`networkx.DiGraph` otherwise.\n",
        "\n",
        "    Args:\n",
        "        data (torch_geometric.data.Data): The data object.\n",
        "        node_attrs (iterable of str, optional): The node attributes to be\n",
        "            copied. (default: :obj:`None`)\n",
        "        edge_attrs (iterable of str, optional): The edge attributes to be\n",
        "            copied. (default: :obj:`None`)\n",
        "        to_undirected (bool, optional): If set to :obj:`True`, will return a\n",
        "            a :obj:`networkx.Graph` instead of a :obj:`networkx.DiGraph`. The\n",
        "            undirected graph will correspond to the upper triangle of the\n",
        "            corresponding adjacency matrix. (default: :obj:`False`)\n",
        "        remove_self_loops (bool, optional): If set to :obj:`True`, will not\n",
        "            include self loops in the resulting graph. (default: :obj:`False`)\n",
        "    \"\"\"\n",
        "\n",
        "    if to_undirected:\n",
        "        G = nx.Graph()\n",
        "    else:\n",
        "        G = nx.DiGraph()\n",
        "\n",
        "    G.add_nodes_from(range(data.num_nodes))\n",
        "\n",
        "    values = {}\n",
        "    for key, item in data:\n",
        "        if torch.is_tensor(item):\n",
        "            values[key] = item.squeeze().tolist()\n",
        "        else:\n",
        "            values[key] = item\n",
        "        if isinstance(values[key], (list, tuple)) and len(values[key]) == 1:\n",
        "            values[key] = item[0]\n",
        "\n",
        "    for i, (u, v) in enumerate(data.edge_index.t().tolist()):\n",
        "\n",
        "        if to_undirected and v > u:\n",
        "            continue\n",
        "\n",
        "        if remove_self_loops and u == v:\n",
        "            continue\n",
        "\n",
        "        G.add_edge(u, v)\n",
        "        for key in edge_attrs if edge_attrs is not None else []:\n",
        "            G[u][v][key] = values[key][i]\n",
        "\n",
        "    for key in node_attrs if node_attrs is not None else []:\n",
        "        for i, feat_dict in G.nodes(data=True):\n",
        "            feat_dict.update({key: values[key][i]})\n",
        "\n",
        "    return G\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jN7t426gf4wg"
      },
      "source": [
        "import random\n",
        "import heapq\n",
        "import importlib\n",
        "import math\n",
        "import multiprocessing as mp\n",
        "import time\n",
        "from functools import lru_cache\n",
        "\n",
        "import cvxpy as cvx\n",
        "import networkit as nk\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import ot\n",
        "\n",
        "import logging\n",
        "import community as community_louvain\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "from functools import partial, partialmethod\n",
        "\n",
        "logging.TRACE = logging.DEBUG + 5\n",
        "logging.addLevelName(logging.TRACE, 'TRACE')\n",
        "logging.Logger.trace = partialmethod(logging.Logger.log, logging.TRACE)\n",
        "logging.trace = partial(logging.log, logging.TRACE)\n",
        "\n",
        "logger = logging.getLogger(\"GraphRicciCurvature\")"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzAOhwDeg9oh"
      },
      "source": [
        "def set_verbose(verbose=\"ERROR\"):\n",
        "    \"\"\"Set up the verbose level of the GraphRicciCurvature.\n",
        "    Parameters\n",
        "    ----------\n",
        "    verbose : {\"INFO\", \"TRACE\",\"DEBUG\",\"ERROR\"}\n",
        "        Verbose level. (Default value = \"ERROR\")\n",
        "            - \"INFO\": show only iteration process log.\n",
        "            - \"TRACE\": show detailed iteration process log.\n",
        "            - \"DEBUG\": show all output logs.\n",
        "            - \"ERROR\": only show log if error happened.\n",
        "    \"\"\"\n",
        "    if verbose == \"INFO\":\n",
        "        logger.setLevel(logging.INFO)\n",
        "    elif verbose == \"TRACE\":\n",
        "        logger.setLevel(logging.TRACE)\n",
        "    elif verbose == \"DEBUG\":\n",
        "        logger.setLevel(logging.DEBUG)\n",
        "    elif verbose == \"ERROR\":\n",
        "        logger.setLevel(logging.ERROR)\n",
        "    else:\n",
        "        print('Incorrect verbose level, option:[\"INFO\",\"DEBUG\",\"ERROR\"], use \"ERROR instead.\"')\n",
        "        logger.setLevel(logging.ERROR)\n",
        "\n",
        "\n",
        "def cut_graph_by_cutoff(G_origin, cutoff, weight=\"weight\"):\n",
        "    \"\"\"Remove graph's edges with \"weight\" greater than \"cutoff\".\n",
        "    Parameters\n",
        "    ----------\n",
        "    G_origin : NetworkX graph\n",
        "        A graph with ``weight`` as Ricci flow metric to cut.\n",
        "    cutoff : float\n",
        "        A threshold to remove all edges with \"weight\" greater than it.\n",
        "    weight : str\n",
        "        The edge weight used as Ricci flow metric. (Default value = \"weight\")\n",
        "    Returns\n",
        "    -------\n",
        "    G: NetworkX graph\n",
        "        A graph with edges cut by given cutoff value.\n",
        "    \"\"\"\n",
        "    assert nx.get_edge_attributes(G_origin, weight), \"No edge weight detected, abort.\"\n",
        "\n",
        "    G = G_origin.copy()\n",
        "    edge_trim_list = []\n",
        "    for n1, n2 in G.edges():\n",
        "        if G[n1][n2][weight] > cutoff:\n",
        "            edge_trim_list.append((n1, n2))\n",
        "    G.remove_edges_from(edge_trim_list)\n",
        "    return G\n",
        "\n",
        "\n",
        "def get_rf_metric_cutoff(G_origin, weight=\"weight\", cutoff_step=0.025, drop_threshold=0.01):\n",
        "    \"\"\"Get good clustering cutoff points for Ricci flow metric by detect the change of modularity while removing edges.\n",
        "    Parameters\n",
        "    ----------\n",
        "    G_origin : NetworkX graph\n",
        "        A graph with \"weight\" as Ricci flow metric to cut.\n",
        "    weight : str\n",
        "        The edge weight used as Ricci flow metric. (Default value = \"weight\")\n",
        "    cutoff_step : float\n",
        "        The step size to find the good cutoff points.\n",
        "    drop_threshold : float\n",
        "        At least drop this much to considered as a drop for good_cut.\n",
        "    Returns\n",
        "    -------\n",
        "    good_cuts : list of float\n",
        "        A list of possible cutoff point, usually we use the first one as the best cut.\n",
        "    \"\"\"\n",
        "\n",
        "    G = G_origin.copy()\n",
        "    modularity, ari = [], []\n",
        "    maxw = max(nx.get_edge_attributes(G, weight).values())\n",
        "    cutoff_range = np.arange(maxw, 1, -cutoff_step)\n",
        "\n",
        "    for cutoff in cutoff_range:\n",
        "        G = cut_graph_by_cutoff(G, cutoff, weight=weight)\n",
        "        # Get connected component after cut as clustering\n",
        "        clustering = {c: idx for idx, comp in enumerate(nx.connected_components(G)) for c in comp}\n",
        "        # Compute modularity\n",
        "        modularity.append(community_louvain.modularity(clustering, G, weight))\n",
        "\n",
        "    good_cuts = []\n",
        "    mod_last = modularity[-1]\n",
        "\n",
        "    # check drop from 1 -> maxw\n",
        "    for i in range(len(modularity) - 1, 0, -1):\n",
        "        mod_now = modularity[i]\n",
        "        if mod_last > mod_now > 1e-4 and abs(mod_last - mod_now) / mod_last > drop_threshold:\n",
        "            logger.trace(\"Cut detected: cut:%f, diff:%f, mod_now:%f, mod_last:%f\" % (\n",
        "                cutoff_range[i+1], mod_last - mod_now, mod_now, mod_last))\n",
        "            good_cuts.append(cutoff_range[i+1])\n",
        "        mod_last = mod_now\n",
        "\n",
        "    return good_cuts"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQzG5icshFUt"
      },
      "source": [
        "print\n",
        "EPSILON = 1e-7  # to prevent divided by zero\n",
        "\n",
        "# ---Shared global variables for multiprocessing used.---\n",
        "_Gk = nk.graph.Graph()\n",
        "_alpha = 0.5\n",
        "_weight = \"weight\"\n",
        "_method = \"Sinkhorn\"\n",
        "_base = math.e\n",
        "_exp_power = 2\n",
        "_proc = mp.cpu_count()\n",
        "_cache_maxsize = 1000000\n",
        "_shortest_path = \"all_pairs\"\n",
        "_nbr_topk = 1000\n",
        "_apsp = {}\n",
        "# -------------------------------------------------------\n",
        "\n",
        "#@lru_cache(_cache_maxsize)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVBhSjIphI0H"
      },
      "source": [
        "\n",
        "def _get_single_node_neighbors_distributions(node, direction=\"successors\"):\n",
        "    \"\"\"Get the neighbor density distribution of given node `node`.\n",
        "    Parameters\n",
        "    ----------\n",
        "    node : int\n",
        "        Node index in Networkit graph `_Gk`.\n",
        "    direction : {\"predecessors\", \"successors\"}\n",
        "        Direction of neighbors in directed graph. (Default value: \"successors\")\n",
        "    Returns\n",
        "    -------\n",
        "    distributions : lists of float\n",
        "        Density distributions of neighbors up to top `_nbr_topk` nodes.\n",
        "    nbrs : lists of int\n",
        "        Neighbor index up to top `_nbr_topk` nodes.\n",
        "    \"\"\"\n",
        "    if _Gk.isDirected():\n",
        "        if direction == \"predecessors\":\n",
        "            neighbors = list(_Gk.iterInNeighbors(node))\n",
        "        else:  # successors\n",
        "            neighbors = list(_Gk.iterNeighbors(node))\n",
        "    else:\n",
        "        neighbors = list(_Gk.iterNeighbors(node))\n",
        "\n",
        "    # Get sum of distributions from x's all neighbors\n",
        "    heap_weight_node_pair = []\n",
        "    for nbr in neighbors:\n",
        "        if direction == \"predecessors\":\n",
        "            w = _base ** (-_Gk.weight(nbr, node) ** _exp_power)\n",
        "        else:  # successors\n",
        "            w = _base ** (-_Gk.weight(node, nbr) ** _exp_power)\n",
        "\n",
        "        if len(heap_weight_node_pair) < _nbr_topk:\n",
        "            heapq.heappush(heap_weight_node_pair, (w, nbr))\n",
        "        else:\n",
        "            heapq.heappushpop(heap_weight_node_pair, (w, nbr))\n",
        "\n",
        "    nbr_edge_weight_sum = sum([x[0] for x in heap_weight_node_pair])\n",
        "\n",
        "    if not neighbors:\n",
        "        # No neighbor, all mass stay at node\n",
        "        return [1], [node]\n",
        "\n",
        "    if nbr_edge_weight_sum > EPSILON:\n",
        "        # Sum need to be not too small to prevent divided by zero\n",
        "        distributions = [(1.0 - _alpha) * w / nbr_edge_weight_sum for w, _ in heap_weight_node_pair]\n",
        "    else:\n",
        "        # Sum too small, just evenly distribute to every neighbors\n",
        "        logger.warning(\"Neighbor weight sum too small, list:\", heap_weight_node_pair)\n",
        "        distributions = [(1.0 - _alpha) / len(heap_weight_node_pair)] * len(heap_weight_node_pair)\n",
        "\n",
        "    nbr = [x[1] for x in heap_weight_node_pair]\n",
        "    return distributions + [_alpha], nbr + [node]\n",
        "\n",
        "\n",
        "def _distribute_densities(source, target):\n",
        "    \"\"\"Get the density distributions of source and target node, and the cost (all pair shortest paths) between\n",
        "    all source's and target's neighbors. Notice that only neighbors with top `_nbr_topk` edge weights.\n",
        "    Parameters\n",
        "    ----------\n",
        "    source : int\n",
        "        Source node index in Networkit graph `_Gk`.\n",
        "    target : int\n",
        "        Target node index in Networkit graph `_Gk`.\n",
        "    Returns\n",
        "    -------\n",
        "    x : (m,) numpy.ndarray\n",
        "        Source's density distributions, includes source and source's neighbors.\n",
        "    y : (n,) numpy.ndarray\n",
        "        Target's density distributions, includes source and source's neighbors.\n",
        "    d : (m, n) numpy.ndarray\n",
        "        Shortest path matrix.\n",
        "    \"\"\"\n",
        "\n",
        "    # Distribute densities for source and source's neighbors as x\n",
        "    t0 = time.time()\n",
        "\n",
        "    if _Gk.isDirected():\n",
        "        x, source_topknbr = _get_single_node_neighbors_distributions(source, \"predecessors\")\n",
        "    else:\n",
        "        x, source_topknbr = _get_single_node_neighbors_distributions(source, \"successors\")\n",
        "\n",
        "    # Distribute densities for target and target's neighbors as y\n",
        "    y, target_topknbr = _get_single_node_neighbors_distributions(target, \"successors\")\n",
        "\n",
        "    logger.debug(\"%8f secs density distribution for edge.\" % (time.time() - t0))\n",
        "\n",
        "    # construct the cost dictionary from x to y\n",
        "    t0 = time.time()\n",
        "\n",
        "    if _shortest_path == \"pairwise\":\n",
        "        d = []\n",
        "        for src in source_topknbr:\n",
        "            tmp = []\n",
        "            for tgt in target_topknbr:\n",
        "                tmp.append(_source_target_shortest_path(src, tgt))\n",
        "            d.append(tmp)\n",
        "        d = np.array(d)\n",
        "    else:  # all_pairs\n",
        "        d = _apsp[np.ix_(source_topknbr, target_topknbr)]  # transportation matrix\n",
        "\n",
        "    x = np.array([x]).T  # the mass that source neighborhood initially owned\n",
        "    y = np.array([y]).T  # the mass that target neighborhood needs to received\n",
        "\n",
        "    logger.debug(\"%8f secs density matrix construction for edge.\" % (time.time() - t0))\n",
        "\n",
        "    return x, y, d\n",
        "\n",
        "\n",
        "@lru_cache(_cache_maxsize)\n",
        "def _source_target_shortest_path(source, target):\n",
        "    \"\"\"Compute pairwise shortest path from `source` to `target` by BidirectionalDijkstra via Networkit.\n",
        "    Parameters\n",
        "    ----------\n",
        "    source : int\n",
        "        Source node index in Networkit graph `_Gk`.\n",
        "    target : int\n",
        "        Target node index in Networkit graph `_Gk`.\n",
        "    Returns\n",
        "    -------\n",
        "    length : float\n",
        "        Pairwise shortest path length.\n",
        "    \"\"\"\n",
        "\n",
        "    length = nk.distance.BidirectionalDijkstra(_Gk, source, target).run().getDistance()\n",
        "    assert length < 1e300, \"Shortest path between %d, %d is not found\" % (source, target)\n",
        "    return length\n",
        "\n",
        "\n",
        "def _get_all_pairs_shortest_path():\n",
        "    \"\"\"Pre-compute all pairs shortest paths of the assigned graph `_Gk`.\"\"\"\n",
        "    logger.trace(\"Start to compute all pair shortest path.\")\n",
        "\n",
        "    global _Gk\n",
        "\n",
        "    t0 = time.time()\n",
        "    apsp = nk.distance.APSP(_Gk).run().getDistances()\n",
        "    logger.trace(\"%8f secs for all pair by NetworKit.\" % (time.time() - t0))\n",
        "\n",
        "    return np.array(apsp)\n",
        "\n",
        "\n",
        "def _optimal_transportation_distance(x, y, d):\n",
        "    \"\"\"Compute the optimal transportation distance (OTD) of the given density distributions by CVXPY.\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : (m,) numpy.ndarray\n",
        "        Source's density distributions, includes source and source's neighbors.\n",
        "    y : (n,) numpy.ndarray\n",
        "        Target's density distributions, includes source and source's neighbors.\n",
        "    d : (m, n) numpy.ndarray\n",
        "        Shortest path matrix.\n",
        "    Returns\n",
        "    -------\n",
        "    m : float\n",
        "        Optimal transportation distance.\n",
        "    \"\"\"\n",
        "\n",
        "    t0 = time.time()\n",
        "    rho = cvx.Variable((len(y), len(x)))  # the transportation plan rho\n",
        "\n",
        "    # objective function d(x,y) * rho * x, need to do element-wise multiply here\n",
        "    obj = cvx.Minimize(cvx.sum(cvx.multiply(np.multiply(d.T, x.T), rho)))\n",
        "\n",
        "    # \\sigma_i rho_{ij}=[1,1,...,1]\n",
        "    source_sum = cvx.sum(rho, axis=0, keepdims=True)\n",
        "    constrains = [rho @ x == y, source_sum == np.ones((1, (len(x)))), 0 <= rho, rho <= 1]\n",
        "    prob = cvx.Problem(obj, constrains)\n",
        "\n",
        "    m = prob.solve()  # change solver here if you want\n",
        "    # solve for optimal transportation cost\n",
        "\n",
        "    logger.debug(\"%8f secs for cvxpy. \\t#source_nbr: %d, #target_nbr: %d\" % (time.time() - t0, len(x), len(y)))\n",
        "\n",
        "    return m\n",
        "\n",
        "\n",
        "def _sinkhorn_distance(x, y, d):\n",
        "    \"\"\"Compute the approximate optimal transportation distance (Sinkhorn distance) of the given density distributions.\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : (m,) numpy.ndarray\n",
        "        Source's density distributions, includes source and source's neighbors.\n",
        "    y : (n,) numpy.ndarray\n",
        "        Target's density distributions, includes source and source's neighbors.\n",
        "    d : (m, n) numpy.ndarray\n",
        "        Shortest path matrix.\n",
        "    Returns\n",
        "    -------\n",
        "    m : float\n",
        "        Sinkhorn distance, an approximate optimal transportation distance.\n",
        "    \"\"\"\n",
        "    t0 = time.time()\n",
        "    m = ot.sinkhorn2(x, y, d, 1e-1, method='sinkhorn')[0]\n",
        "    logger.debug(\n",
        "        \"%8f secs for Sinkhorn. dist. \\t#source_nbr: %d, #target_nbr: %d\" % (time.time() - t0, len(x), len(y)))\n",
        "\n",
        "    return m\n",
        "\n",
        "\n",
        "def _average_transportation_distance(source, target):\n",
        "    \"\"\"Compute the average transportation distance (ATD) of the given density distributions.\n",
        "    Parameters\n",
        "    ----------\n",
        "    source : int\n",
        "        Source node index in Networkit graph `_Gk`.\n",
        "    target : int\n",
        "        Target node index in Networkit graph `_Gk`.\n",
        "    Returns\n",
        "    -------\n",
        "    m : float\n",
        "        Average transportation distance.\n",
        "    \"\"\"\n",
        "\n",
        "    t0 = time.time()\n",
        "    if _Gk.isDirected():\n",
        "        source_nbr = list(_Gk.iterInNeighbors(source))\n",
        "    else:\n",
        "        source_nbr = list(_Gk.iterNeighbors(source))\n",
        "    target_nbr = list(_Gk.iterNeighbors(target))\n",
        "\n",
        "    share = (1.0 - _alpha) / (len(source_nbr) * len(target_nbr))\n",
        "    cost_nbr = 0\n",
        "    cost_self = _alpha * _apsp[source][target]\n",
        "\n",
        "    for src in source_nbr:\n",
        "        for tgt in target_nbr:\n",
        "            cost_nbr += _apsp[src][tgt] * share\n",
        "\n",
        "    m = cost_nbr + cost_self  # Average transportation cost\n",
        "\n",
        "    logger.debug(\"%8f secs for avg trans. dist. \\t#source_nbr: %d, #target_nbr: %d\" % (time.time() - t0,\n",
        "                                                                                       len(source_nbr),\n",
        "                                                                                       len(target_nbr)))\n",
        "    return m\n",
        "\n",
        "\n",
        "def _compute_ricci_curvature_single_edge(source, target):\n",
        "    \"\"\"Ricci curvature computation for a given single edge.\n",
        "    Parameters\n",
        "    ----------\n",
        "    source : int\n",
        "        Source node index in Networkit graph `_Gk`.\n",
        "    target : int\n",
        "        Target node index in Networkit graph `_Gk`.\n",
        "    Returns\n",
        "    -------\n",
        "    result : dict[(int,int), float]\n",
        "        The Ricci curvature of given edge in dict format. E.g.: {(node1, node2): ricciCurvature}\n",
        "    \"\"\"\n",
        "    # logger.debug(\"EDGE:%s,%s\"%(source,target))\n",
        "    assert source != target, \"Self loop is not allowed.\"  # to prevent self loop\n",
        "\n",
        "    # If the weight of edge is too small, return 0 instead.\n",
        "    if _Gk.weight(source, target) < EPSILON:\n",
        "        logger.warning(\"Zero weight edge detected for edge (%s,%s), return Ricci Curvature as 0 instead.\" %\n",
        "                       (source, target))\n",
        "        return {(source, target): 0}\n",
        "\n",
        "    # compute transportation distance\n",
        "    m = 1  # assign an initial cost\n",
        "    assert _method in [\"OTD\", \"ATD\", \"Sinkhorn\"], \\\n",
        "        'Method %s not found, support method:[\"OTD\", \"ATD\", \"Sinkhorn\"]' % _method\n",
        "    if _method == \"OTD\":\n",
        "        x, y, d = _distribute_densities(source, target)\n",
        "        m = _optimal_transportation_distance(x, y, d)\n",
        "    elif _method == \"ATD\":\n",
        "        m = _average_transportation_distance(source, target)\n",
        "    elif _method == \"Sinkhorn\":\n",
        "        x, y, d = _distribute_densities(source, target)\n",
        "        m = _sinkhorn_distance(x, y, d)\n",
        "\n",
        "    # compute Ricci curvature: k=1-(m_{x,y})/d(x,y)\n",
        "    result = 1 - (m / _Gk.weight(source, target))  # Divided by the length of d(i, j)\n",
        "    logger.debug(\"Ricci curvature (%s,%s) = %f\" % (source, target, result))\n",
        "\n",
        "    return {(source, target): result}\n",
        "\n",
        "\n",
        "def _wrap_compute_single_edge(stuff):\n",
        "    \"\"\"Wrapper for args in multiprocessing.\"\"\"\n",
        "    return _compute_ricci_curvature_single_edge(*stuff)\n",
        "\n",
        "\n",
        "def _compute_ricci_curvature_edges(G: nx.Graph, weight=\"weight\", edge_list=[],\n",
        "                                   alpha=0.5, method=\"OTD\",\n",
        "                                   base=math.e, exp_power=2, proc=mp.cpu_count(), chunksize=None, cache_maxsize=1000000,\n",
        "                                   shortest_path=\"all_pairs\", nbr_topk=1000):\n",
        "    \"\"\"Compute Ricci curvature for edges in  given edge lists.\n",
        "    Parameters\n",
        "    ----------\n",
        "    G : NetworkX graph\n",
        "        A given directional or undirectional NetworkX graph.\n",
        "    weight : str\n",
        "        The edge weight used to compute Ricci curvature. (Default value = \"weight\")\n",
        "    edge_list : list of edges\n",
        "        The list of edges to compute Ricci curvature, set to [] to run for all edges in G. (Default value = [])\n",
        "    alpha : float\n",
        "        The parameter for the discrete Ricci curvature, range from 0 ~ 1.\n",
        "        It means the share of mass to leave on the original node.\n",
        "        E.g. x -> y, alpha = 0.4 means 0.4 for x, 0.6 to evenly spread to x's nbr.\n",
        "        (Default value = 0.5)\n",
        "    method : {\"OTD\", \"ATD\", \"Sinkhorn\"}\n",
        "        The optimal transportation distance computation method. (Default value = \"OTD\")\n",
        "        Transportation method:\n",
        "            - \"OTD\" for Optimal Transportation Distance,\n",
        "            - \"ATD\" for Average Transportation Distance.\n",
        "            - \"Sinkhorn\" for OTD approximated Sinkhorn distance.  (faster)\n",
        "    base : float\n",
        "        Base variable for weight distribution. (Default value = `math.e`)\n",
        "    exp_power : float\n",
        "        Exponential power for weight distribution. (Default value = 0)\n",
        "    proc : int\n",
        "        Number of processor used for multiprocessing. (Default value = `cpu_count()`)\n",
        "    chunksize : int\n",
        "        Chunk size for multiprocessing, set None for auto decide. (Default value = `None`)\n",
        "    cache_maxsize : int\n",
        "        Max size for LRU cache for pairwise shortest path computation.\n",
        "        Set this to `None` for unlimited cache. (Default value = 1000000)\n",
        "    shortest_path : {\"all_pairs\",\"pairwise\"}\n",
        "        Method to compute shortest path. (Default value = `all_pairs`)\n",
        "    nbr_topk : int\n",
        "        Only take the top k edge weight neighbors for density distribution.\n",
        "        Smaller k run faster but the result is less accurate. (Default value = 1000)\n",
        "    Returns\n",
        "    -------\n",
        "    output : dict[(int,int), float]\n",
        "        A dictionary of edge Ricci curvature. E.g.: {(node1, node2): ricciCurvature}.\n",
        "    \"\"\"\n",
        "\n",
        "    logger.trace(\"Number of nodes: %d\" % G.number_of_nodes())\n",
        "    logger.trace(\"Number of edges: %d\" % G.number_of_edges())\n",
        "\n",
        "    if not nx.get_edge_attributes(G, weight):\n",
        "        logger.info('Edge weight not detected in graph, use \"weight\" as default edge weight.')\n",
        "        for (v1, v2) in G.edges():\n",
        "            G[v1][v2][weight] = 1.0\n",
        "\n",
        "    # ---set to global variable for multiprocessing used.---\n",
        "    global _Gk\n",
        "    global _alpha\n",
        "    global _weight\n",
        "    global _method\n",
        "    global _base\n",
        "    global _exp_power\n",
        "    global _proc\n",
        "    global _cache_maxsize\n",
        "    global _shortest_path\n",
        "    global _nbr_topk\n",
        "    global _apsp\n",
        "    # -------------------------------------------------------\n",
        "\n",
        "    _Gk = nk.nxadapter.nx2nk(G, weightAttr=weight)\n",
        "    _alpha = alpha\n",
        "    _weight = weight\n",
        "    _method = method\n",
        "    _base = base\n",
        "    _exp_power = exp_power\n",
        "    _proc = proc\n",
        "    _cache_maxsize = cache_maxsize\n",
        "    _shortest_path = shortest_path\n",
        "    _nbr_topk = nbr_topk\n",
        "\n",
        "    # Construct nx to nk dictionary\n",
        "    nx2nk_ndict, nk2nx_ndict = {}, {}\n",
        "    for idx, n in enumerate(G.nodes()):\n",
        "        nx2nk_ndict[n] = idx\n",
        "        nk2nx_ndict[idx] = n\n",
        "\n",
        "    if _shortest_path == \"all_pairs\":\n",
        "        # Construct the all pair shortest path dictionary\n",
        "        # if not _apsp:\n",
        "        _apsp = _get_all_pairs_shortest_path()\n",
        "\n",
        "    #args = []\n",
        "    if edge_list:\n",
        "      # count = 0\n",
        "      # for source, target in edge_list:\n",
        "      #   count += 1\n",
        "      #   if not (count%1000):\n",
        "      #     print(count)\n",
        "      #     args += [(nx2nk_ndict[source], nx2nk_ndict[target])]\n",
        "\n",
        "      args = [(nx2nk_ndict[source], nx2nk_ndict[target]) for source, target in edge_list]\n",
        "    else:\n",
        "      # count = 0\n",
        "      # for source, target in edge_list:\n",
        "      #   count += 1\n",
        "      #   if not (count%1000):\n",
        "      #     print(count)\n",
        "      #     args += [(nx2nk_ndict[source], nx2nk_ndict[target])]\n",
        "      args = [(nx2nk_ndict[source], nx2nk_ndict[target]) for source, target in G.edges()]\n",
        "\n",
        "    # Start compute edge Ricci curvature\n",
        "    t0 = time.time()\n",
        "\n",
        "    with mp.get_context('fork').Pool(processes=_proc) as pool:\n",
        "        # WARNING: Now only fork works, spawn will hang.\n",
        "\n",
        "        # Decide chunksize following method in map_async\n",
        "        if chunksize is None:\n",
        "            chunksize, extra = divmod(len(args), proc * 4)\n",
        "            if extra:\n",
        "                chunksize += 1\n",
        "\n",
        "        # Compute Ricci curvature for edges\n",
        "        result = pool.imap_unordered(_wrap_compute_single_edge, args, chunksize=chunksize)\n",
        "        pool.close()\n",
        "        pool.join()\n",
        "\n",
        "    # Convert edge index from nk back to nx for final output\n",
        "    output = {}\n",
        "    #count = 0\n",
        "    for rc in result:\n",
        "        # count += 1\n",
        "        # if not (count%100):\n",
        "        #   print(count)\n",
        "        for k in list(rc.keys()):\n",
        "            output[(nk2nx_ndict[k[0]], nk2nx_ndict[k[1]])] = rc[k]\n",
        "\n",
        "    logger.info(\"%8f secs for Ricci curvature computation.\" % (time.time() - t0))\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "def _compute_ricci_curvature(G: nx.Graph, weight=\"weight\", **kwargs):\n",
        "    \"\"\"Compute Ricci curvature of edges and nodes.\n",
        "    The node Ricci curvature is defined as the average of node's adjacency edges.\n",
        "    Parameters\n",
        "    ----------\n",
        "    G : NetworkX graph\n",
        "        A given directional or undirectional NetworkX graph.\n",
        "    weight : str\n",
        "        The edge weight used to compute Ricci curvature. (Default value = \"weight\")\n",
        "    **kwargs\n",
        "        Additional keyword arguments passed to `_compute_ricci_curvature_edges`.\n",
        "    Returns\n",
        "    -------\n",
        "    G: NetworkX graph\n",
        "        A NetworkX graph with \"ricciCurvature\" on nodes and edges.\n",
        "    \"\"\"\n",
        "\n",
        "    # compute Ricci curvature for all edges\n",
        "    edge_ricci = _compute_ricci_curvature_edges(G, weight=weight, **kwargs)\n",
        "\n",
        "    # Assign edge Ricci curvature from result to graph G\n",
        "    nx.set_edge_attributes(G, edge_ricci, \"ricciCurvature\")\n",
        "\n",
        "    # Compute node Ricci curvature\n",
        "    for n in G.nodes():\n",
        "        rc_sum = 0  # sum of the neighbor Ricci curvature\n",
        "        if G.degree(n) != 0:\n",
        "            for nbr in G.neighbors(n):\n",
        "                if 'ricciCurvature' in G[n][nbr]:\n",
        "                    rc_sum += G[n][nbr]['ricciCurvature']\n",
        "\n",
        "            # Assign the node Ricci curvature to be the average of node's adjacency edges\n",
        "            G.nodes[n]['ricciCurvature'] = rc_sum / G.degree(n)\n",
        "            logger.debug(\"node %s, Ricci Curvature = %f\" % (n, G.nodes[n]['ricciCurvature']))\n",
        "\n",
        "    return G\n",
        "\n",
        "\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3V8nSISCeC2"
      },
      "source": [
        "weight=\"weight\"\n",
        "alpha=0.5\n",
        "method=\"Sinkhorn\"\n",
        "base=math.e\n",
        "exp_power=2\n",
        "proc=mp.cpu_count()\n",
        "chunksize=None\n",
        "shortest_path=\"all_pairs\"\n",
        "cache_maxsize=1000000\n",
        "nbr_topk=1000\n",
        "verbose=\"ERROR\""
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vrvs3wX92BDa",
        "outputId": "067b9228-d0f7-45db-86cd-3fdf9aa70af4"
      },
      "source": [
        "data = dataset[0]\n",
        "data.edge_index = to_undirected(data.edge_index, data.num_nodes)\n",
        "gdir = to_networkx(data)\n",
        "len(gdir.edges)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2315598"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ujl9WbUzYbs",
        "outputId": "90a51801-9ecf-428e-e3e7-db0c30bcfa9f"
      },
      "source": [
        "#gdir = to_networkx(dataset[0])\n",
        "remaining_edges = list(gdir.edges)\n",
        "print(len(remaining_edges))\n",
        "count = 0\n",
        "curv_dict = {}\n",
        "while len(remaining_edges):\n",
        "  count += 1\n",
        "  print(count)\n",
        "  if len(remaining_edges) > 12000:\n",
        "    sampled_edges = remaining_edges[:12000]\n",
        "    remaining_edges= remaining_edges[12000:]\n",
        "    sampled_graph = gdir.edge_subgraph(sampled_edges)\n",
        "    ricci_c = _compute_ricci_curvature_edges(G= sampled_graph, weight= weight,\n",
        "                                          alpha= alpha, method= method,\n",
        "                                          base= base, exp_power= exp_power,\n",
        "                                          proc= 10, chunksize=chunksize, cache_maxsize= cache_maxsize,\n",
        "                                          shortest_path= shortest_path, nbr_topk= nbr_topk)\n",
        "    curv_dict.update(ricci_c)\n",
        "    print(len(sampled_edges))\n",
        "    # print(len(remaining_edges))\n",
        "  else:\n",
        "    sampled_edges = remaining_edges\n",
        "    remaining_edges= []\n",
        "    print(len(sampled_edges))\n",
        "    # print(len(remaining_edges))\n",
        "    sampled_graph = gdir.edge_subgraph(sampled_edges)\n",
        "    ricci_c = _compute_ricci_curvature_edges(G= sampled_graph, weight= weight,\n",
        "                                          alpha= alpha, method= method,\n",
        "                                          base= base, exp_power= exp_power,\n",
        "                                          proc= 10, chunksize=chunksize, cache_maxsize= cache_maxsize,\n",
        "                                          shortest_path= shortest_path, nbr_topk= nbr_topk)\n",
        "    curv_dict.update(ricci_c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2315598\n",
            "1\n",
            "12000\n",
            "2\n",
            "12000\n",
            "3\n",
            "12000\n",
            "4\n",
            "12000\n",
            "5\n",
            "12000\n",
            "6\n",
            "12000\n",
            "7\n",
            "12000\n",
            "8\n",
            "12000\n",
            "9\n",
            "12000\n",
            "10\n",
            "12000\n",
            "11\n",
            "12000\n",
            "12\n",
            "12000\n",
            "13\n",
            "12000\n",
            "14\n",
            "12000\n",
            "15\n",
            "12000\n",
            "16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBfYeHqkvx0Q",
        "outputId": "7f8745de-add9-4a71-87cb-a28afe5074b4"
      },
      "source": [
        "!pip install ujson\n",
        "import ujson"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ujson\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/4e/50e8e4cf5f00b537095711c2c86ac4d7191aed2b4fffd5a19f06898f6929/ujson-4.0.2-cp37-cp37m-manylinux1_x86_64.whl (179kB)\n",
            "\r\u001b[K     |█▉                              | 10kB 14.9MB/s eta 0:00:01\r\u001b[K     |███▋                            | 20kB 16.2MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 30kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 40kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 51kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 61kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 71kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 81kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 92kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 102kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 112kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 122kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 133kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 143kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 153kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 163kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 174kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 184kB 4.4MB/s \n",
            "\u001b[?25hInstalling collected packages: ujson\n",
            "Successfully installed ujson-4.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuzXh9Uqvdbz"
      },
      "source": [
        "with open('curvature_full.txt', 'w') as file:\n",
        "    file.write(ujson.dumps(curv_dict))"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfN8_9ahs2-j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}